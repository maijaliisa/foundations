{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping many pages + Using Selenium\n",
    "\n",
    "## The pages we'll be looking at\n",
    "\n",
    "If I wanted to read specific information about a specfic mine, it takes a few steps. **Do these steps with your browser before you try any programming.**\n",
    "\n",
    "1. Visit the [Mine Data Retrieval System](https://arlweb.msha.gov/drs/drshome.htm)\n",
    "2. Scroll down to **Mine Identification Number (ID) Search**\n",
    "3. Type in a mine ID number, such as `3503598`, click **Search**\n",
    "4. I'm on a page! It lists the MINE NAME and MINE OWNER.\n",
    "\n",
    "After searching for and finding a mine, I can use this page to **find reports about this mine**. Some of the reports are on accidents, violations, inspections, health samples and more. To get those reports:\n",
    "\n",
    "1. Search for a mine (if you haven't already)\n",
    "2. Scroll down and change **Beginning Date** to `1/1/1995` (violation reports begin in 1995, accidents begin in 1983)\n",
    "3. Select the report type of `Violations`\n",
    "4. Click **Get Report**\n",
    "5. I'm on a page! It lists ALL OF THE MINE'S VIOLATIONS.\n",
    "\n",
    "By changing the report type you're searching for you can find all sorts of different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Researching mine information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation \n",
    "\n",
    "### When you search for information on a specific mine, what URL should Selenium visit first?\n",
    "\n",
    "- *TIP: the answer is NOT `https://arlweb.msha.gov/drs/ASP/BasicMineInfonew.asp`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://arlweb.msha.gov/drs/drshome.htm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you identify the text field we're going to type the Mine ID into?\n",
    "\n",
    "Selenium can find elements by:\n",
    "\n",
    "- name\n",
    "- Class\n",
    "- ID\n",
    "- CSS selector (**ASK ME WHAT THIS IS** if you don't know)\n",
    "- XPath (**ASK ME WHAT THIS IS** because you definitely don't know)\n",
    "- Link text\n",
    "- Partial link text\n",
    "\n",
    "So in other words, what's unique about this element?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# //*[@id=\"inputdrs\"] (the xpath for the Mine ID area on the main search page )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you identify the search button we're going to click, or the form we're going to submit?\n",
    "\n",
    "Selenium can submit forms by either\n",
    "\n",
    "- Selecting the form and using `.submit()`, or\n",
    "- Selecting the button and using `.click()`\n",
    "\n",
    "You only need to be able to get **one, not both.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# //*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Selenium to search using the mine ID `3901432`. Get me the operator's name by scraping.\n",
    "\n",
    "- *TIP: You can find elements/text using Selenium, or use BeautifulSoup with `doc = BeautifulSoup(driver.page_source)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.get('https://arlweb.msha.gov/drs/drshome.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"123acad9cc936fa1fd94266bccf57e1f\", element=\"0.15989126644044038-1\")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_finder = driver.find_element_by_name('MineId')\n",
    "id_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_finder.send_keys(3901432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input ')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#don't need beautiful soup because we are just looking for one element on the HTML page, not a bunch of information! \n",
    "#from bs4 import BeautifulSoup\n",
    "#doc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#doc.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Krueger Brothers Gravel & Dirt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine_name = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[1]/tbody/tr[3]/td[2]/font/b')\n",
    "mine_name.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .apply to find data about SEVERAL mines\n",
    "\n",
    "The file `mines-subset.csv` has a list of mine IDs. We're going to scrape the operator's name for each of those mines.\n",
    "\n",
    "### Open up `mines-subset.csv` and save it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'mines-subset.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fc4a7ec4ddd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mines-subset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'mines-subset.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#we are dealing with pdfs so we are going to use dataframes in pandas\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('mines-subset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up `mines-subset.csv` in a text editor, then look at your dataframe. Is something different about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: mines-subset.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "#running terminal commands inside notebook\n",
    "!cat mines-subset.csv\n",
    "#this creates jsut one column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the operator's name for each of those mines and print it\n",
    "\n",
    "- *TIP: use .apply and a function*\n",
    "- *TIP: If you need help with .apply, look at the \"Using apply in pandas\" notebook *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run a function through every single row  \n",
    "def name_scrape(row):\n",
    "    driver.get('https://arlweb.msha.gov/drs/drshome.htm')\n",
    "    id_finder = driver.find_element_by_name('MineID')\n",
    "    id_finder.send_keys(row['id'])\n",
    "    search = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input ')\n",
    "    mine_name = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[1]/tbody/tr[3]/td[2]/font/b')\n",
    "    print(row['id'])\n",
    "    print('-----')\n",
    "    \n",
    "driver = webdriver.Chrome()     \n",
    "df.apply(name_scrape, axis = 1)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the operator's name and save it into a new column\n",
    "\n",
    "- *TIP: Use .apply and a function*\n",
    "- *TIP: Remember to use `return`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-47c8fa8e738a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scrape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def name_scrape(row):\n",
    "    driver.get('https://arlweb.msha.gov/drs/drshome.htm')\n",
    "    id_finder = driver.find_element_by_name('MineId')\n",
    "    id_finder.send_keys(row['id'])\n",
    "    search = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input ')\n",
    "    mine_name = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[1]/tbody/tr[3]/td[2]/font/b')\n",
    "    print(row['id'])\n",
    "    print('-----')\n",
    "    \n",
    "driver = webdriver.Chrome()     \n",
    "df.apply(name_scrape, axis = 1)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Researching mine violations\n",
    "\n",
    "Read the very top again to remember how to find mine violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you search for a mine's violations, what URL is Selenium going to start on?\n",
    "\n",
    "- *TIP: `requests` can send form data to load in the middle of a bunch of steps, but Selenium has to start at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://arlweb.msha.gov/drs/drshome.htm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you're searching for violations from the Mine Information page, how are you going to identify the \"Beginning Date\" field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#//*[@id=\"content\"]/form[1]/table[2]/tbody/tr[2]/td/font/input[1] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you're searching for violations from the Mine Information page, how are you going to identify the \"Violations\" button?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#violations: //*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### When you're searching for violations from the Mine Information page, how are you going to identify the form or the button to click to get a list of the violations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# //*[@id=\"content\"]/form[1]/table[3]/tbody/tr[3]/td[2]/input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mine ID `3901432`, scrape all of their violations since 1/1/1995\n",
    "\n",
    "**Save this into a CSV called `3901432-violations.csv`.** This CSV must include the following fields:\n",
    "\n",
    "- Citation number\n",
    "- Case number\n",
    "- Standard violated\n",
    "- Link to standard\n",
    "- Proposed penalty\n",
    "- Amount paid to date\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- *TIP: It's probably worth it to print them all first, then save them to a CSV once you know it's all working.*\n",
    "- *TIP: You'll use the parent pattern - get the ROWS first (tr), then loop through and get the TABLE CELLS (td)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    driver.get('https://arlweb.msha.gov/drs/drshome.htm')\n",
    "    id_finder = driver.find_element_by_name('MineId')\n",
    "    id_finder.send_keys(row['id'])\n",
    "    search = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input ')\n",
    "    search.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[2]/tbody/tr[2]/td/font/input[1] ')\n",
    "date.send_keys('1/1/1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pick_violation = driver.find_elemement_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input')\n",
    "pick_violation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_report_button = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "get_report_button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a dictionary with beautiful soup...can't use Selenium because will be going through a lot of rows..\n",
    "#xpath is just for unique elements on a page \n",
    "# Citation number\n",
    "#Case number\n",
    "#Standard violated\n",
    "#Link to standard\n",
    "#Proposed penalty\n",
    "#Amount paid to date \n",
    "\n",
    "#In beautifulsoup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "doc2 = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "violations = doc.find_all('tr', class_ = 'draviols')\n",
    "for violations in violations:\n",
    "    print('this is a violation')\n",
    "    all_cells = violations.find_all('td')\n",
    "    print('citation number', all_cells[2].text)\n",
    "    print('case number', all_cells[3].text)\n",
    "    print('Standard Violation:', all_cells[10].find('a')['href'])\n",
    "    print('Proposed penalty', all_cells[11].text)\n",
    "    print('Amount paid', all_cells[14].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving csv\n",
    "#1 make an empty list\n",
    "#2 every time through loop, create dictionary\n",
    "#3 add dictionary to the list\n",
    "#4 when done, convert list to a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .apply to save mine data for SEVERAL mines\n",
    "\n",
    "The file `mines-subset.csv` has a list of mine IDs. We're going to scrape the operator's name for each of those mines.\n",
    "\n",
    "### Open up `mines-subset.csv` and save it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_violations = []\n",
    "for violations in all_violations:\n",
    "    violation = {}\n",
    "    print('this is a violation')\n",
    "    all_cells = violations.find_all('td')\n",
    "    violation['citation number'], all_cells[2].text.strip()\n",
    "    violation['case number'], all_cells[3].text.strip()\n",
    "    violation['Standard Violation:'], all_cells[10].find('a')['href']\n",
    "    violation['Proposed penalty'], all_cells[11].text.strip()\n",
    "    violation['Amount paid'], all_cells[14].text.strip()\n",
    "    violation.append(all_violations) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(all_violations)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('mines.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the violations for each mine\n",
    "\n",
    "**Save each mine's violations into separate CSV files.** Each CSV file must include the following fields:\n",
    "\n",
    "- Citation number\n",
    "- Case number\n",
    "- Standard violated\n",
    "- Link to standard\n",
    "- Proposed penalty\n",
    "- Amount paid to date\n",
    "\n",
    "Make sure you are saving them into **separate files.** It might be nice to name them after the mine id.\n",
    "\n",
    "- *TIP: Use .apply for this*\n",
    "- *TIP: Print out the ID before you start scraping. That way you can take that ID and search manually to see if there is anything weird about the results.*\n",
    "- *TIP: If you need help with .apply, look at the \"Using apply in pandas\" notebook \n",
    "- *TIP: It's probably worth it to print the fields first, then save them to a CSV once you know it's all working.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
